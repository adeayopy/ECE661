{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_1_points=np.array([[553, 1234],[2647, 1292],[2834, 2601], [245, 2500],[943, 1493], [2331, 2270],[1281, 1155],[1166, 2356]])\n",
    "pic_2_points=np.array([[152, 1723],[2108, 874],[2914, 1867],[569, 3032],[713, 1759],[2360, 1795], [871, 1292],[1425, 2399]])\n",
    "\n",
    "\n",
    "def display_images_with_correspondences(img1, img2, points1, points2):\n",
    "    \"\"\"\n",
    "    Displays two images side-by-side with lines connecting corresponding points.\n",
    "\n",
    "    Args:\n",
    "        img1: The first image.\n",
    "        img2: The second image.\n",
    "        points1: A list of points in the first image.\n",
    "        points2: A list of corresponding points in the second image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine images horizontally\n",
    "    combined_img = np.hstack((img1, img2))\n",
    "\n",
    "    # Calculate offset for the second image\n",
    "    offset = img1.shape[1]\n",
    "\n",
    "    # Draw lines between corresponding points\n",
    "    for p1, p2 in zip(points1, points2):\n",
    "        cv2.line(combined_img, (p1[0], p1[1]), (p2[0] + offset, p2[1]), (0, 255, 0), 10)\n",
    "\n",
    "    # Display the combined image\n",
    "    plt.imshow(cv2.cvtColor(combined_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    # plt.savefig(f'Correspondence')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Load images and corresponding points\n",
    "img1 = cv2.imread('Pic_1.jpg')\n",
    "img2 = cv2.imread('Pic_2.jpg')\n",
    "\n",
    "\n",
    "# Display the images with lines\n",
    "display_images_with_correspondences(img1, img2, pic_1_points, pic_2_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_points(points):\n",
    "    \"\"\"\n",
    "    Normalize a set of 2D points to improve numerical stability.\n",
    "    \"\"\"\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std = np.std(points)\n",
    "    \n",
    "    T = np.array([\n",
    "        [1/std, 0, -mean[0]/std],\n",
    "        [0, 1/std, -mean[1]/std],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    normalized_points = np.dot(T, np.vstack((points.T, np.ones(points.shape[0]))))\n",
    "    return normalized_points[:2].T, T\n",
    "\n",
    "def eight_point_algorithm(x, x_prime):\n",
    "    \"\"\"\n",
    "    Computes the Fundamental Matrix F using the normalized 8-point algorithm.\n",
    "    \"\"\"\n",
    "    # Normalize the points\n",
    "    x, T = normalize_points(x)\n",
    "    x_prime, T_prime = normalize_points(x_prime)\n",
    "    \n",
    "    # Construct matrix A for the linear equation\n",
    "    A = np.zeros((len(x), 9))\n",
    "    for i in range(len(x)):\n",
    "        A[i] = [\n",
    "            x_prime[i, 0] * x[i, 0], x_prime[i, 0] * x[i, 1], x_prime[i, 0],\n",
    "            x_prime[i, 1] * x[i, 0], x_prime[i, 1] * x[i, 1], x_prime[i, 1],\n",
    "            x[i, 0], x[i, 1], 1\n",
    "        ]\n",
    "    \n",
    "    # Solve for F using SVD\n",
    "    _, _, vh = svd(A)\n",
    "    F = vh[-1].reshape(3, 3)\n",
    "    \n",
    "    # Enforce rank-2 constraint by setting the smallest singular value to 0\n",
    "    u, d, vh = svd(F)\n",
    "    d[-1] = 0\n",
    "    F = u @ np.diag(d) @ vh\n",
    "    \n",
    "    # Denormalize F\n",
    "    F = T_prime.T @ F @ T\n",
    "    return F\n",
    "\n",
    "\n",
    "F = eight_point_algorithm(pic_1_points, pic_2_points)\n",
    "print(\"Fundamental Matrix F:\\n\", F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_epipoles(F):\n",
    "\n",
    "    # Compute the SVD of F\n",
    "    _, _, vh = np.linalg.svd(F)\n",
    "    # Right epipole (e') is the last row of V (transpose of vh)\n",
    "    e_prime = vh[-1, :]\n",
    "    e_prime = e_prime / e_prime[2]  # Normalize to ensure homogeneous coordinate\n",
    "    \n",
    "    # Left epipole (e) is the last column of U\n",
    "    u, _, _ = np.linalg.svd(F.T)\n",
    "    e = u[:, -1]\n",
    "    e = e / e[2]  # Normalize to ensure homogeneous coordinate\n",
    "    \n",
    "    return e, e_prime\n",
    "\n",
    "def compute_projection_matrices(e_prime, F):\n",
    "\n",
    "    # Initialize P for the left image in canonical form\n",
    "    P = np.array([[1, 0, 0, 0], \n",
    "                  [0, 1, 0, 0], \n",
    "                  [0, 0, 1, 0]])\n",
    "    \n",
    "    # Compute e_prime_matrix (skew-symmetric matrix of e_prime)\n",
    "    e_prime_matrix = np.array([\n",
    "        [0, -e_prime[2], e_prime[1]],\n",
    "        [e_prime[2], 0, -e_prime[0]],\n",
    "        [-e_prime[1], e_prime[0], 0]\n",
    "    ])\n",
    "    \n",
    "    # Compute P' as [e_prime_matrix * F | e_prime]\n",
    "    P_prime = np.hstack((e_prime_matrix @ F, e_prime.reshape(3, 1)))\n",
    "\n",
    "    # Ensure that both matrices have a rank of 3\n",
    "    assert np.linalg.matrix_rank(P) == 3, \"P does not have full rank\"\n",
    "    assert np.linalg.matrix_rank(P_prime) == 3, \"P_prime does not have full rank\"\n",
    "    \n",
    "    return P, P_prime\n",
    "\n",
    "\n",
    "e, e_prime = compute_epipoles(F)\n",
    "P, P_prime = compute_projection_matrices(e_prime, F)\n",
    "\n",
    "print(\"Left Epipole (e):\", e)\n",
    "print(\"Right Epipole (e'):\", e_prime)\n",
    "print(\"Projection Matrix P:\\n\", P)\n",
    "print(\"Projection Matrix P':\\n\", P_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate(P, P_prime, x_points, x_prime_points):\n",
    "    world_points = []\n",
    "    for x, x_prime in zip(x_points, x_prime_points):\n",
    "        A = np.zeros((4, 4))\n",
    "        A[0] = x[0] * P[2] - P[0]\n",
    "        A[1] = x[1] * P[2] - P[1]\n",
    "        A[2] = x_prime[0] * P_prime[2] - P_prime[0]\n",
    "        A[3] = x_prime[1] * P_prime[2] - P_prime[1]\n",
    "        \n",
    "        _, _, vh = np.linalg.svd(A)\n",
    "        world_point = vh[-1]\n",
    "        world_points.append(world_point / world_point[3])  # Convert to homogeneous coordinates\n",
    "\n",
    "    return np.array(world_points)[:, :3]\n",
    "\n",
    "def world2image(P, x):\n",
    "    x_h = np.append(x, 1)\n",
    "    img_point = P @ x_h\n",
    "    return img_point[:2] / img_point[2]\n",
    "\n",
    "def cost_function(params, x_points, x_prime_points):\n",
    "    # Reshape params into P_prime\n",
    "    P_prime = params.reshape(3, 4)\n",
    "    \n",
    "    # Project points and calculate residuals\n",
    "    world_points = triangulate(P, P_prime, x_points, x_prime_points)\n",
    "    x_prime_projected = np.array([world2image(P_prime, pt) for pt in world_points])\n",
    "    residuals = (x_prime_points - x_prime_projected).ravel()\n",
    "    return residuals\n",
    "\n",
    "# Perform LM optimization to refine P_prime\n",
    "initial_params = P_prime.ravel()\n",
    "result = least_squares(cost_function, initial_params, args=(pic_1_points, pic_2_points), method='lm')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_prime_optimized = result.x.reshape(3, 4)\n",
    "print(\"Optimized Projection Matrix for Right Image (P_prime):\\n\", P_prime_optimized)\n",
    "\n",
    "\n",
    "def refine_translation(P_prime_ref):\n",
    "    # Step 1: Compute t_ref from the refined projection matrix\n",
    "    t_ref = P_prime_ref[:, -1]\n",
    "\n",
    "    # Step 2: Generate the skew-symmetric matrix for t_ref\n",
    "    t_ref_matrix = np.array([\n",
    "        [0, -t_ref[2], t_ref[1]],\n",
    "        [t_ref[2], 0, -t_ref[0]],\n",
    "        [-t_ref[1], t_ref[0], 0]\n",
    "    ])\n",
    "\n",
    "    # Step 3: Compute the refined fundamental matrix F_ref\n",
    "    F_ref = t_ref_matrix @ P_prime_ref[:, :3]\n",
    "    F_ref = F_ref / F_ref[-1, -1]  # Normalize\n",
    "\n",
    "    # Step 4: Compute the refined right epipole e_prime_ref\n",
    "    e_prime_ref = compute_epipoles(F_ref)[1]\n",
    "    return e_prime_ref\n",
    "\n",
    "e_prime_optimized =refine_translation(P_prime_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_prime(img, e_prime):\n",
    "\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Compute T1\n",
    "    T1 = np.array([[1.0, 0.0, -w/2],\n",
    "                   [0.0, 1.0, -h/2],\n",
    "                   [0.0, 0.0, 1.0]])\n",
    "\n",
    "    # Compute T2\n",
    "    T2 = np.array([[1.0, 0.0, w/2],\n",
    "                   [0.0, 1.0, h/2],\n",
    "                   [0.0, 0.0, 1.0]])\n",
    "\n",
    "    # Compute x0, y0, and theta\n",
    "    x0 = w/2\n",
    "    y0 = h/2\n",
    "    e_prime = e_prime / e_prime[-1]\n",
    "    ex = e_prime[0]\n",
    "    ey = e_prime[1]\n",
    "    theta = np.arctan(-(ey-y0)/(ex-x0))\n",
    "\n",
    "    # Compute R and f\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                   [np.sin(theta), np.cos(theta), 0],\n",
    "                   [0, 0, 1]])\n",
    "    f = np.abs((ex-x0)*np.cos(theta) - (ey-y0)*np.sin(theta))\n",
    "\n",
    "    # Compute G\n",
    "    G = np.array([[1, 0, 0],\n",
    "                   [0, 1, 0],\n",
    "                   [-1/f, 0, 1]])\n",
    "\n",
    "    # Compute H_prime\n",
    "    H_prime = np.matmul(T2, np.matmul(G, np.matmul(R, T1)))\n",
    "\n",
    "    return H_prime, f\n",
    "\n",
    "H_right, f_right=get_H_prime(img2, e_prime_optimized)\n",
    "print(H_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rectify_left_image(P, P_prime, H_prime, x_points, x_prime_points):\n",
    "\n",
    "    # Compute the pseudo-inverse of P\n",
    "    P_pinv = np.linalg.pinv(P)\n",
    "\n",
    "    # Step 1: Compute the initial homography H_0\n",
    "    H_0 = np.matmul(P_prime, P_pinv)\n",
    "\n",
    "    # Step 2: Transform points in the left image using H_0\n",
    "    x_hat = np.array([np.dot(H_0, np.hstack((x, 1))) for x in x_points])\n",
    "    x_hat = (x_hat.T / x_hat[:, 2]).T  # Normalize to homogeneous coordinates\n",
    "\n",
    "    # Step 3: Transform points in the right image using H_prime\n",
    "    x_prime_hat = np.array([np.dot(H_prime, np.hstack((x_prime, 1))) for x_prime in x_prime_points])\n",
    "    x_prime_hat = (x_prime_hat.T / x_prime_hat[:, 2]).T  # Normalize to homogeneous coordinates\n",
    "\n",
    "    # Step 4: Solve for affine transformation H_a to align x_hat with x_prime_hat\n",
    "    A = []\n",
    "    b = []\n",
    "    for i in range(len(x_hat)):\n",
    "        x, y, _ = x_hat[i]\n",
    "        x_prime, y_prime, _ = x_prime_hat[i]\n",
    "\n",
    "        # Create equations for x and y coordinates\n",
    "        A.append([x, y, 1, 0, 0, 0])\n",
    "        A.append([0, 0, 0, x, y, 1])\n",
    "        b.append(x_prime)\n",
    "        b.append(y_prime)\n",
    "\n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "\n",
    "    # Solve for the affine transformation parameters\n",
    "    a, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n",
    "    \n",
    "    # Step 5: Construct H_a as an affine transformation matrix\n",
    "    H_a = np.array([\n",
    "        [a[0], a[1], a[2]],\n",
    "        [a[3], a[4], a[5]],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # Step 6: Compute the final homography H for the left image\n",
    "    H = H_a @ H_0\n",
    "    return H\n",
    "\n",
    "H_left = rectify_left_image(P, P_prime_optimized,H_right, pic_1_points, pic_2_points)\n",
    "print(H_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify_images(img1, img2, H, Hp, w=3024, h=4032):\n",
    "\n",
    "\n",
    "    # Rectify images using cv2.warpPerspective\n",
    "    rectified1 = cv2.warpPerspective(img1, H, (w, h))\n",
    "    rectified2 = cv2.warpPerspective(img2, Hp, (w, h))\n",
    "    # print(rectified1)\n",
    "    # Display the rectified images\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(rectified1, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Rectified Image 1')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(rectified2, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Rectified Image 2')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('Rectified correspondence')\n",
    "    plt.show()\n",
    "\n",
    "    return rectified1, rectified2\n",
    "\n",
    "rectified_img1, rectified_img2 = rectify_images(img1, img2, H_left, H_right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rectify_images_with_correspondences(img1, img2, H, Hp, points1, points2, w, h):\n",
    "\n",
    "\n",
    "    # Rectify images using cv2.warpPerspective\n",
    "    rectified1 = cv2.warpPerspective(img1, H, (w, h))\n",
    "    rectified2 = cv2.warpPerspective(img2, Hp, (w, h))\n",
    "\n",
    "    # Apply homography to points to get rectified points\n",
    "    points1_rectified = cv2.perspectiveTransform(np.array([points1], dtype='float32'), H)[0]\n",
    "    points2_rectified = cv2.perspectiveTransform(np.array([points2], dtype='float32'), Hp)[0]\n",
    "\n",
    "    # Plot rectified images side by side with correspondences\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(cv2.cvtColor(rectified1, cv2.COLOR_BGR2RGB))\n",
    "    ax[0].set_title('Rectified Image 1')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(cv2.cvtColor(rectified2, cv2.COLOR_BGR2RGB))\n",
    "    ax[1].set_title('Rectified Image 2')\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    # Draw lines for corresponding points\n",
    "    for pt1, pt2 in zip(points1_rectified, points2_rectified):\n",
    "        # Convert points to integers\n",
    "        pt1 = tuple(map(int, pt1))\n",
    "        pt2 = tuple(map(int, pt2))\n",
    "\n",
    "        # Draw a circle for the point in both images\n",
    "        ax[0].plot(pt1[0], pt1[1], 'ro')  # red point in left image\n",
    "        ax[1].plot(pt2[0], pt2[1], 'ro')  # red point in right image\n",
    "\n",
    "        # Draw a line on both images to show correspondence (across same row if rectified correctly)\n",
    "        ax[0].hlines(pt1[1], 0, w, colors='blue', linestyles='dotted', linewidth=1.5)\n",
    "        ax[1].hlines(pt2[1], 0, w, colors='blue', linestyles='dotted', linewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Rectified correspondence epipolar')\n",
    "    plt.show()\n",
    "\n",
    "    return points1_rectified, points2_rectified, rectified1, rectified2\n",
    "\n",
    "rectified_point1, rectified_point2, rectified_img1, rectified_img2 = rectify_images_with_correspondences(img1, img2, H_left, H_right, pic_1_points, pic_2_points, 3024, 4032)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_correspondences(rectified_img1, rectified_img2, points1_rectified, search_window=5):\n",
    "\n",
    "    h, w, _ = rectified_img2.shape\n",
    "    correspondences = []\n",
    "\n",
    "    for pt1 in points1_rectified:\n",
    "        x1, y1 = int(pt1[0]), int(pt1[1])\n",
    "\n",
    "        # Define a search window in the second image\n",
    "        y2_start = max(0, y1 - search_window)\n",
    "        y2_end = min(h, y1 + search_window + 1)\n",
    "\n",
    "        # Extract the search row and the corresponding row from both images\n",
    "        row1 = rectified_img1[y1, :]\n",
    "        row2 = rectified_img2[y2_start:y2_end, :]\n",
    "\n",
    "        # Calculate SSD for each pixel in the search window\n",
    "        best_ssd = float('inf')\n",
    "        best_x2 = x1  # Default to the same x-coordinate if no match is found\n",
    "        for y2_idx, y2 in enumerate(range(y2_start, y2_end)):\n",
    "            ssd = np.sum((row1[x1 - 5:x1 + 5] - row2[y2_idx, x1 - 5:x1 + 5]) ** 2)\n",
    "            if ssd < best_ssd:\n",
    "                best_ssd = ssd\n",
    "                best_x2 = x1  # Corresponding x-coordinate (for rectified images)\n",
    "\n",
    "        correspondences.append((best_x2, y1))\n",
    "\n",
    "    return correspondences\n",
    "\n",
    "\n",
    "\n",
    "def draw_correspondences(rectified_img1, rectified_img2, points1_rectified, corresponding_points):\n",
    "\n",
    "    # Combine both images side-by-side\n",
    "    combined_img = np.hstack((rectified_img1, rectified_img2))\n",
    "\n",
    "    # Offset for the second image (to adjust x-coordinates when drawing lines)\n",
    "    offset_x = rectified_img1.shape[1]\n",
    "\n",
    "    # Convert to RGB if the images are grayscale\n",
    "    if len(combined_img.shape) == 2:\n",
    "        combined_img = cv2.cvtColor(combined_img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Draw points and lines\n",
    "    for pt1, pt2 in zip(points1_rectified, corresponding_points):\n",
    "        x1, y1 = int(pt1[0]), int(pt1[1])\n",
    "        x2, y2 = int(pt2[0] + offset_x), int(pt2[1])  # Offset x-coordinate for the second image\n",
    "\n",
    "        # Draw points\n",
    "        cv2.circle(combined_img, (x1, y1), 5, (0, 255, 0), -1)  # Green for points in Image 1\n",
    "        cv2.circle(combined_img, (x2, y2), 5, (255, 0, 0), -1)  # Blue for points in Image 2\n",
    "        \n",
    "        bgr_color = (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256))\n",
    "\n",
    "        # Draw connecting lines\n",
    "        cv2.line(combined_img, (x1, y1), (x2, y2), bgr_color, 3)  # Yellow lines\n",
    "\n",
    "    # Display the combined image with correspondences\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(cv2.cvtColor(combined_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Correspondences Between Rectified Images\")\n",
    "    plt.savefig((\"Rectified Images canny\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interest_points = cv2.goodFeaturesToTrack(cv2.cvtColor(rectified_img1, cv2.COLOR_BGR2GRAY), 500, 0.01, 10)\n",
    "interest_points = np.int0(interest_points).reshape(-1, 2)\n",
    "\n",
    "# Find correspondences in the second rectified image\n",
    "corresponding_points = find_correspondences(rectified_img1, rectified_img2, interest_points)\n",
    "\n",
    "# Visualize correspondences with lines\n",
    "draw_correspondences(rectified_img1, rectified_img2, interest_points, corresponding_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def back_project_to_world(canny_points1, canny_points2, P_left, P_right):\n",
    "\n",
    "    def triangulate_point(pt1, pt2, P1, P2):\n",
    "        \"\"\"Triangulate a single pair of points.\"\"\"\n",
    "        A = np.zeros((4, 4))\n",
    "        A[0] = pt1[0] * P1[2] - P1[0]\n",
    "        A[1] = pt1[1] * P1[2] - P1[1]\n",
    "        A[2] = pt2[0] * P2[2] - P2[0]\n",
    "        A[3] = pt2[1] * P2[2] - P2[1]\n",
    "\n",
    "        _, _, vh = np.linalg.svd(A)\n",
    "        world_point = vh[-1]\n",
    "        return world_point[:3] / world_point[3]  # Convert to 3D (inhomogeneous) coordinates\n",
    "\n",
    "    world_points = []\n",
    "    for pt1, pt2 in zip(canny_points1, canny_points2):\n",
    "        world_point = triangulate_point(pt1, pt2, P_left, P_right)\n",
    "        world_points.append(world_point)\n",
    "\n",
    "    return np.array(world_points)\n",
    "\n",
    "world_points = back_project_to_world(rectified_point1, rectified_point2, P, P_prime_optimized)\n",
    "\n",
    "\n",
    "e_n, e_prime_n = compute_epipoles(F_n)\n",
    "P_n, P_prime_n = compute_projection_matrices(e_prime_n, F_n)\n",
    "\n",
    "# Perform LM optimization to refine P_prime\n",
    "initial_params = P_prime_n.ravel()\n",
    "result_n = least_squares(cost_function, initial_params, args=(interest_points, corresponding_points), method='lm')\n",
    "P_prime_optimized_n = result_n.x.reshape(3, 4)\n",
    "print(\"Optimized Projection Matrix for Right Image (P_prime):\\n\", P_prime_optimized_n)\n",
    "\n",
    "# Back-project points into 3D world space\n",
    "world_points = back_project_to_world(pic_1_points, pic_2_points, P_n, P_prime_optimized_n)\n",
    "\n",
    "# Display the resulting 3D points\n",
    "# print(\"3D World Points:\")\n",
    "# print(world_points)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image1_points = pic_1_points\n",
    "image2_points = pic_2_points\n",
    "\n",
    "\n",
    "# Convert images from BGR (OpenCV default) to RGB for Matplotlib\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot Image 1 with points\n",
    "ax1 = fig.add_subplot(3, 2, 1)\n",
    "ax1.imshow(img1)\n",
    "ax1.scatter(image1_points[:, 0], image1_points[:, 1], color='r', label='Image 1 Points')\n",
    "ax1.set_title(\"Image 1\")\n",
    "ax1.axis('off')\n",
    "\n",
    "# Plot Image 2 with points\n",
    "ax2 = fig.add_subplot(3, 2, 2)\n",
    "ax2.imshow(img2)\n",
    "ax2.scatter(image2_points[:, 0], image2_points[:, 1], color='g', label='Image 2 Points')\n",
    "ax2.set_title(\"Image 2\")\n",
    "ax2.axis('off')\n",
    "\n",
    "# Combine both images with corresponding lines\n",
    "ax3 = fig.add_subplot(3, 1, 2)\n",
    "ax3.imshow(np.hstack((img1, img2)))\n",
    "offset = img1.shape[1]  # Offset for the second image\n",
    "for i in range(len(image1_points)):\n",
    "    ax3.plot(\n",
    "        [image1_points[i, 0], image2_points[i, 0] + offset],\n",
    "        [image1_points[i, 1], image2_points[i, 1]],\n",
    "        color='b'\n",
    "    )\n",
    "ax3.scatter(image1_points[:, 0], image1_points[:, 1], color='r', label='Image 1 Points')\n",
    "ax3.scatter(image2_points[:, 0] + offset, image2_points[:, 1], color='g', label='Image 2 Points')\n",
    "ax3.set_title(\"Image Correspondences\")\n",
    "ax3.axis('off')\n",
    "\n",
    "# 3D Plot of reconstructed points\n",
    "ax4 = fig.add_subplot(3, 1, 3, projection='3d')\n",
    "ax4.scatter(world_points[:, 0], world_points[:, 1], world_points[:, 2], color='m', label='World Points')\n",
    "\n",
    "# Draw lines from world points to the corresponding image points\n",
    "for i in range(len(world_points)):\n",
    "    # Line to Image 1\n",
    "    ax4.plot(\n",
    "        [world_points[i, 0], -4],  # Arbitrary connections to image plane\n",
    "        [world_points[i, 1], -3],\n",
    "        [world_points[i, 2], -1.5],\n",
    "        color='r'\n",
    "    )\n",
    "    # Line to Image 2\n",
    "    ax4.plot(\n",
    "        [world_points[i, 0], -4],\n",
    "        [world_points[i, 1], -3],\n",
    "        [world_points[i, 2], -1.5],\n",
    "        color='g'\n",
    "    )\n",
    "\n",
    "ax4.set_title(\"3D Reconstruction\")\n",
    "ax4.set_xlabel(\"X\")\n",
    "ax4.set_ylabel(\"Y\")\n",
    "ax4.set_zlabel(\"Z\")\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_census(imgL, imgR, dMax):\n",
    "\n",
    "    \"\"\" Apply the census transform to estimate the disparity map. \"\"\"\n",
    "\n",
    "    # Ensure we use grayscale images\n",
    "    if len(imgL.shape) > 2:\n",
    "        imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
    "    if len(imgR.shape) > 2:\n",
    "        imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    halfWinSz = int(M / 2)\n",
    "    borderSz = dMax + halfWinSz\n",
    "\n",
    "    height, width = imgL.shape\n",
    "    dMap = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Optimize by reducing window size access repetition and loop restructuring\n",
    "    for rowL in range(borderSz, height - borderSz):\n",
    "        for colL in range(borderSz, width - borderSz):\n",
    "            windowL = imgL[rowL - halfWinSz:rowL + halfWinSz + 1, colL - halfWinSz:colL + halfWinSz + 1].ravel()\n",
    "            minCost = float('inf')\n",
    "            bestMatch = 0\n",
    "\n",
    "            # Iterate over disparity values to find best match\n",
    "            for d in range(dMax + 1):\n",
    "                colR = colL - d\n",
    "                if colR < borderSz:\n",
    "                    break\n",
    "\n",
    "                windowR = imgR[rowL - halfWinSz:rowL + halfWinSz + 1, colR - halfWinSz:colR + halfWinSz + 1].ravel()\n",
    "                cost = np.sum(windowL != windowR)  # Hamming distance as a simple matching cost\n",
    "\n",
    "                if cost < minCost:\n",
    "                    minCost = cost\n",
    "                    bestMatch = d\n",
    "\n",
    "            dMap[rowL, colL] = bestMatch\n",
    "\n",
    "    return dMap\n",
    "    # Implement the census transform here (or use existing function)\n",
    "    # This is a placeholder for census-based stereo matching\n",
    "    # Here, we assume that this function returns a disparity map\n",
    "    \n",
    "\n",
    "# Load images and ground truth disparity map\n",
    "imgL = cv2.imread('Task3Images/Task3Images/im2.png', cv2.IMREAD_GRAYSCALE)\n",
    "imgR = cv2.imread('Task3Images/Task3Images/im6.png', cv2.IMREAD_GRAYSCALE)\n",
    "gtDMap = cv2.imread('Task3Images/Task3Images/disp2.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Convert ground truth disparity map to float32 and adjust\n",
    "gtDMap = gtDMap.astype(np.float32)\n",
    "gtDMap /= 4  # Adjust by dividing by 4 as per the instruction\n",
    "gtDMap = gtDMap.astype(np.uint8)\n",
    "dMax = int(gtDMap.max())\n",
    "M=31\n",
    "# Estimate disparity map using Census transform\n",
    "dMap = apply_census(imgL, imgR, dMax)\n",
    "\n",
    "# Normalize dMap for viewing\n",
    "dMapView = (dMap / np.max(dMap) * 255).astype(np.uint8)\n",
    "\n",
    "# Compute the error map\n",
    "error = np.abs(dMap.astype(np.int16) - gtDMap.astype(np.int16)).astype(np.uint8)\n",
    "\n",
    "# Binary mask where error is <= delta\n",
    "delta = 2  # You can adjust the delta value as needed\n",
    "errorMask = np.zeros_like(error)\n",
    "errorMask[error <= delta] = 255  # Set mask value to 255 where error is <= delta\n",
    "\n",
    "# Compute the number of valid pixels (non-black pixels in ground truth)\n",
    "validMask = gtDMap > 0  # Pixels where ground truth disparity is valid\n",
    "N = np.count_nonzero(validMask)  # Total number of valid pixels\n",
    "\n",
    "# Compute the d1 and d2 error ratios\n",
    "validError = error[validMask]\n",
    "d1Error = np.count_nonzero(validError > 1) / N\n",
    "d2Error = np.count_nonzero(validError > 2) / N\n",
    "\n",
    "# Compute the accuracy\n",
    "correctMatches = np.count_nonzero((error <= delta) & validMask)  # Pixels with error <= delta\n",
    "accuracy = correctMatches / N  # Ratio of correct matches to valid pixels\n",
    "\n",
    "print(f'd1 Error: {d1Error:.2%}')\n",
    "print(f'd2 Error: {d2Error:.2%}')\n",
    "print(f'Accuracy: {accuracy:.2%}')\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Show the disparity map\n",
    "plt.subplot(121)\n",
    "plt.imshow(dMapView, cmap='gray')\n",
    "plt.title('Estimated Disparity Map')\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the error mask (highlighting challenging regions)\n",
    "plt.subplot(122)\n",
    "plt.imshow(errorMask, cmap='gray')\n",
    "plt.title('Binary Error Mask')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cv2.imwrite(f'error_mask M={M}, delta= {delta} d1d2= {d1Error, d2Error}.png', errorMask)\n",
    "cv2.imwrite(f'Disparity map M={M}, delta= {delta} d1d2= {d1Error, d2Error}.png', dMapView)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envece",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
